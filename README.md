# ml-simple-works

머신 러닝 관련 작은 주제들을 선정해서 jupyter notebook으로 실습을 겸하여 내용을 정리하고 있습니다.

**한국어로 찾아보기 힘든 내용**을 선택하여 가능한 자세하게 풀어적고 있으며 

누구나 쉽게 읽을 수 있도록 작성하고 있습니다.

ipynb 파일은 github에서 직접 보면 수식이 깨지고 예쁘게 나오지 않기 때문에

nbviewer를 통해 보거나 https://metamath1.github.io/ 에서 확인하는 것을 추천합니다.

그리고 PC에서만 보시길 권해드립니다. 모바일에서는 수식이 보기 좋지 않아서......


## 지금까지 정리된 내용
- 다변수 가우시안 확률분포multi-variable normal에서 사전확률분포prior, 사후확률분포posterior, 조건부확률분포conditional와 주변확률분포marginal : Pattern Recognition and Machine Learning - Chap. 2-1 , PRML/prml-chap2.ipynb

- 야코비안Jacobian과 치환적분, sampling/double-integral.ipynb

- 베이즈정리와 정규분포의 곱, fitting/product-of-gaussian.ipynb

- 150줄로 된 간단한 네트워크Simple network with 150 lines, simplenet.ipynb/simplenet.ipynb

- Change of continuous random variable - PRML EX-1.4 보충, GAN/change_of_variable.ipynb

- 나이브 베이즈Naive Bayse - 밑바닥부터 시작하는 데이터 과학Data Science from Scratch 보충 설명, naive/naive.ipynb

- 퍼셉트론 테스트와 수렴정리Perceptron test and its convergence theorem, perceptron/perceptron.ipynb

- GANs 기초 - 엔트로피, Keras로 구현한 1D GANs 그리고 Goodfellow 논문 겉핥기, GAN/GANs.ipynb

- CNN 역전파를 이해하는 가장 쉬운 방법The easiest way to understand CNN backpropagation, https://metamath1.github.io/cnn/index.html
